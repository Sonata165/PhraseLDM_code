# Training config for LitOneBarLatentDiffusion

model:
  class_path: experiments.diffusion_prior.lit_ldms.LitPhraseLDM
  init_args:
    model_config:
      sample_size: 512 # n_bar * 4 for now
      in_channels: 128
      num_layers: 6
      attention_head_dim: 32
      num_attention_heads: 16
      num_key_value_attention_heads: 8
      out_channels: 128
      cross_attention_dim: 128
      time_proj_dim: 128
      global_states_input_dim: 128
      cross_attention_input_dim: 128
    mode: phr
    lr: 5e-4
    scale_factor: 2.8357462882995605
    overfit_debug: False
    debug_temp_dir: null
    length_condition: True
    length_bucket_size: 10
    # from_pretrained_ldm_fp: /data1/longshen/Results/AccGenResults/diffusion_prior/phr_latent/full_song/baseline/tb_logs/version_12/checkpoints/step_step=200000.ckpt

# Data module config (example, adjust as needed)
data:
  class_path: datasets.pop909_dataset.POP909DataModule
  init_args:
    jsonl_fp_train: /data1/longshen/Datasets/Piano/POP909/latents/song_level_phr_seq_with_annot/train_data.pt
    jsonl_fp_val: /data1/longshen/Datasets/Piano/POP909/latents/song_level_phr_seq_with_annot/valid_data.pt
    batch_size: 32
    num_workers: 4
    pin_memory: true
    val_split: 0.1
    dataset_class: POP909FullSongPhrLatentDataset
    dataset_kwargs:
      scaling_factor: 2.8357462882995605
      debug_temp_dir: null
      augmentation_type: 'none'
      pad_type: 'max_len'
      max_len: 512

trainer:
  logger:
    class_path: pytorch_lightning.loggers.tensorboard.TensorBoardLogger
    init_args:
      save_dir: /data1/longshen/Results/AccGenResults/diffusion_prior/phr_latent/full_song/baseline_lencond/lr5e-4_bs32
      name: tb_logs
      default_hp_metric: false
    # class_path: pytorch_lightning.loggers.wandb.WandbLogger
    # init_args:
    #   project: accgen
    #   name: baseline_lencond
  check_val_every_n_epoch: 50
  log_every_n_steps: 50
  # max_epochs: 10000
  max_steps: 300000 # 50000 before introducing fid and diversity
  accelerator: gpu
  devices: -1
  precision: 32
  gradient_clip_val: 0.5
  default_root_dir: /data1/longshen/Results/AccGenResults/diffusion_prior/phr_latent/full_song/baseline_lencond/lr5e-4_bs32
  callbacks:
    - class_path: pytorch_lightning.callbacks.LearningRateMonitor
    - class_path: pytorch_lightning.callbacks.ModelSummary
      init_args:
        max_depth: 2
    # - class_path: pytorch_lightning.callbacks.ModelCheckpoint
    #   init_args:
    #     monitor: val_sample_fid
    #     mode: min
    #     save_top_k: 10
    #     filename: '{epoch}_{step}_{val_loss:.4f}_{val_sample_fid:.1f}'
    #     save_last: true
    # 每 10k step 存一个（不看 metric）
    - class_path: pytorch_lightning.callbacks.ModelCheckpoint
      init_args:
        every_n_train_steps: 10000
        save_top_k: -1          # 保存所有
        save_last: true
        filename: 'step_{step}'


